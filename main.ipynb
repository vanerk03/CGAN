{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5boLVENeO-Uu"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "import matplotlib.pyplot as plt\n",
        "import tqdm\n",
        "from gan import GAN\n",
        "from cgan import CGAN\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import sys\n",
        "import platform\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sF_gVtkoO-Uu",
        "outputId": "60305801-c696-41fc-f35c-f7db8c7ffb08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python Platform: Windows-10-10.0.22621-SP0\n",
            "PyTorch Version: 1.12.1+cpu\n",
            "\n",
            "Python 3.10.1 (tags/v3.10.1:2cd268a, Dec  6 2021, 19:10:37) [MSC v.1929 64 bit (AMD64)]\n",
            "GPU is NOT AVAILABLE\n",
            "MPS (Apple Metal) is NOT AVAILABLE\n",
            "Target device is cpu\n"
          ]
        }
      ],
      "source": [
        "# What version of Python do you have?\n",
        "\n",
        "has_gpu = torch.cuda.is_available()\n",
        "has_mps = getattr(torch,'has_mps',False)\n",
        "device = \"mps:0\" if getattr(torch,'has_mps',False) \\\n",
        "    else \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(f\"Python Platform: {platform.platform()}\")\n",
        "print(f\"PyTorch Version: {torch.__version__}\")\n",
        "print()\n",
        "print(f\"Python {sys.version}\")\n",
        "print(\"GPU is\", \"available\" if has_gpu else \"NOT AVAILABLE\")\n",
        "print(\"MPS (Apple Metal) is\", \"AVAILABLE\" if has_mps else \"NOT AVAILABLE\")\n",
        "print(f\"Target device is {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V662lRdSO-Uv",
        "outputId": "94ed3422-5bf0-4660-a424-35dfb3f776db"
      },
      "outputs": [
        {
          "ename": "PicklingError",
          "evalue": "Can't pickle <function <lambda> at 0x0000027F82C4E290>: attribute lookup <lambda> on __main__ failed",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[1;32md:\\NUP\\ML hard\\Project\\CGAN\\main.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/NUP/ML%20hard/Project/CGAN/main.ipynb#X24sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m testset \u001b[39m=\u001b[39m torchvision\u001b[39m.\u001b[39mdatasets\u001b[39m.\u001b[39mMNIST(root\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./data\u001b[39m\u001b[39m'\u001b[39m, train\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/NUP/ML%20hard/Project/CGAN/main.ipynb#X24sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m                                      download\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, transform\u001b[39m=\u001b[39mtransform)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/NUP/ML%20hard/Project/CGAN/main.ipynb#X24sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m testloader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(testset, batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/NUP/ML%20hard/Project/CGAN/main.ipynb#X24sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m                                          shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, num_workers\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/NUP/ML%20hard/Project/CGAN/main.ipynb#X24sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m dataiter \u001b[39m=\u001b[39m \u001b[39miter\u001b[39;49m(testloader)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/NUP/ML%20hard/Project/CGAN/main.ipynb#X24sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m images, labels \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(dataiter)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/NUP/ML%20hard/Project/CGAN/main.ipynb#X24sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m random_image \u001b[39m=\u001b[39m images[\u001b[39m0\u001b[39m]\n",
            "File \u001b[1;32md:\\Programms\\Python3.10\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:444\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator\n\u001b[0;32m    443\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 444\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_iterator()\n",
            "File \u001b[1;32md:\\Programms\\Python3.10\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:390\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    389\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 390\u001b[0m     \u001b[39mreturn\u001b[39;00m _MultiProcessingDataLoaderIter(\u001b[39mself\u001b[39;49m)\n",
            "File \u001b[1;32md:\\Programms\\Python3.10\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1077\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1070\u001b[0m w\u001b[39m.\u001b[39mdaemon \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1071\u001b[0m \u001b[39m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1072\u001b[0m \u001b[39m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1073\u001b[0m \u001b[39m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1074\u001b[0m \u001b[39m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1075\u001b[0m \u001b[39m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1076\u001b[0m \u001b[39m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1077\u001b[0m w\u001b[39m.\u001b[39;49mstart()\n\u001b[0;32m   1078\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_queues\u001b[39m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1079\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_workers\u001b[39m.\u001b[39mappend(w)\n",
            "File \u001b[1;32md:\\Programms\\Python3.10\\lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m _current_process\u001b[39m.\u001b[39m_config\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdaemon\u001b[39m\u001b[39m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mdaemonic processes are not allowed to have children\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_Popen(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    122\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sentinel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen\u001b[39m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[39m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[39m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
            "File \u001b[1;32md:\\Programms\\Python3.10\\lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_context\u001b[39m.\u001b[39;49mget_context()\u001b[39m.\u001b[39;49mProcess\u001b[39m.\u001b[39;49m_Popen(process_obj)\n",
            "File \u001b[1;32md:\\Programms\\Python3.10\\lib\\multiprocessing\\context.py:327\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    325\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    326\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpopen_spawn_win32\u001b[39;00m \u001b[39mimport\u001b[39;00m Popen\n\u001b[1;32m--> 327\u001b[0m     \u001b[39mreturn\u001b[39;00m Popen(process_obj)\n",
            "File \u001b[1;32md:\\Programms\\Python3.10\\lib\\multiprocessing\\popen_spawn_win32.py:93\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     reduction\u001b[39m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 93\u001b[0m     reduction\u001b[39m.\u001b[39;49mdump(process_obj, to_child)\n\u001b[0;32m     94\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     set_spawning_popen(\u001b[39mNone\u001b[39;00m)\n",
            "File \u001b[1;32md:\\Programms\\Python3.10\\lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdump\u001b[39m(obj, file, protocol\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m     \u001b[39m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     ForkingPickler(file, protocol)\u001b[39m.\u001b[39;49mdump(obj)\n",
            "\u001b[1;31mPicklingError\u001b[0m: Can't pickle <function <lambda> at 0x0000027F82C4E290>: attribute lookup <lambda> on __main__ failed"
          ]
        }
      ],
      "source": [
        "EPOCHS = 15\n",
        "batch_size = 32\n",
        "\n",
        "transform = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.Lambda(lambda x: (x - 1/2) * 2)\n",
        "    T.Normalize(mean=(0.5,), std=(0.5,))\n",
        "])\n",
        "\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
        "                                      download=True, transform=transform)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=4)\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
        "                                     download=True, transform=transform)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=4)\n",
        "\n",
        "dataiter = iter(testloader)\n",
        "images, labels = next(dataiter)\n",
        "random_image = images[0]\n",
        "\n",
        "inverse_transform = T.Compose([\n",
        "    T.Lambda(lambda x: (x / 2 + 1/2))\n",
        "])\n",
        "\n",
        "display(T.ToPILImage()(inverse_transform(random_image).squeeze(0)).resize((200, 200)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([784])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMojH0sOO-Uv",
        "outputId": "0b5ed8ed-aa18-4970-a7ae-20b3f5cf717e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 28, 28])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "random_image.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YctRAIA9O-Uw"
      },
      "source": [
        "### FGSM attack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Pji_Mxc8O-Uw"
      },
      "outputs": [],
      "source": [
        "def t_fgsm(img, model, target, eps, criterion):\n",
        "    x = nn.Parameter(img.unsqueeze(0))\n",
        "\n",
        "    pred = model(x)\n",
        "    loss = criterion(pred, target)\n",
        "    loss.backward()\n",
        "\n",
        "    x_ = x - eps / 255 * torch.sign(x.grad)\n",
        "    return torch.clip(x_, min=0, max=1)\n",
        "\n",
        "\n",
        "def u_fgsm(img, model, correct, eps, criterion):\n",
        "    x = nn.Parameter(img.unsqueeze(0))\n",
        "\n",
        "    pred = model(x)\n",
        "    loss = criterion(pred, correct)\n",
        "    loss.backward()\n",
        "\n",
        "    x_ = x + eps / 255 * torch.sign(x.grad)\n",
        "    return torch.clip(x_, min=0, max=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "GAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9yY-v3MZNQp",
        "outputId": "91ecd5a7-1512-4e3a-c326-c087576b0c23"
      },
      "outputs": [],
      "source": [
        "# model = GAN()\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import optuna\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "L1 GAN conditioning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Ellipsis"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "CGAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "import matplotlib.pyplot as plt\n",
        "import tqdm\n",
        "\n",
        "\n",
        "class Reshape(torch.nn.Module):\n",
        "    @property\n",
        "    def device(self):\n",
        "        return next(self.parameters()).device\n",
        "\n",
        "    def __init__(self, *args):\n",
        "        super().__init__()\n",
        "        self.dims = args\n",
        "\n",
        "    def forward(self, input):\n",
        "        return input.view(input.size(0), *self.dims)\n",
        "\n",
        "\n",
        "class G_model(nn.Module):\n",
        "    def __init__(self, device):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.conditional = nn.Sequential(\n",
        "            nn.Linear(50, 49),\n",
        "            Reshape(1, 7, 7)\n",
        "        )\n",
        "\n",
        "        self.reshape_noise = nn.Sequential(\n",
        "            nn.Linear(100, 6272),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            Reshape(128, 7, 7),\n",
        "        )\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.ConvTranspose2d(129, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.ConvTranspose2d(128, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(128, 1, kernel_size=7, stride=1, padding=3),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, X, y):\n",
        "        # create embedding for y\n",
        "        ohe = torch.eye(50, device=self.device)[y]\n",
        "        embed = self.conditional(ohe)\n",
        "        noise = self.reshape_noise(X)\n",
        "        return self.model(torch.cat((embed, noise), dim=1))\n",
        "\n",
        "\n",
        "class D_model(nn.Module):\n",
        "    def __init__(self, n_classes, device):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.conditional = nn.Sequential(\n",
        "            nn.Linear(50, 784),\n",
        "            Reshape(1, 28, 28)\n",
        "        )\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            # two in_channels after concat\n",
        "            nn.Conv2d(in_channels=2, out_channels=128,\n",
        "                      kernel_size=2, stride=2),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(in_channels=128, out_channels=128,\n",
        "                      kernel_size=2, stride=2),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Flatten(start_dim=1),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(6272, n_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, X, y):\n",
        "        # create embedding for y\n",
        "        ohe = torch.eye(50, device=self.device)[y]\n",
        "        embed = self.conditional(ohe)\n",
        "        return self.model(torch.cat((embed, X), dim=1))\n",
        "\n",
        "\n",
        "class GAN(nn.Module):\n",
        "    def __init__(self, device):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        # terrible thing with device, mb this this\n",
        "        self.G = G_model(device).to(device)\n",
        "        self.D = D_model(1, device).to(device)\n",
        "\n",
        "    def forward(self, y):\n",
        "        noise = torch.randn((len(y), 100), device=self.device)\n",
        "        return self.G(noise)\n",
        "\n",
        "    def train(self, dataloader, EPOCHS, verbose=True):\n",
        "        noise_fixed = torch.randn((5, 100), device=self.device)\n",
        "\n",
        "\n",
        "        criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "        optimizer_G = torch.optim.Adam(\n",
        "            self.G.parameters(), lr=3e-4)\n",
        "        optimizer_D = torch.optim.Adam(\n",
        "            self.D.parameters(), lr=3e-4)\n",
        "\n",
        "\n",
        "        batch_size = dataloader.batch_size\n",
        "        G_loss_lst, D_loss_lst = [], []\n",
        "\n",
        "        for epoch in tqdm.notebook.tqdm(range(EPOCHS), desc=\"Epoch\"):\n",
        "\n",
        "            G_running_loss = 0\n",
        "            D_running_loss = 0\n",
        "            for i, (X, y) in enumerate(tqdm.notebook.tqdm(dataloader, leave=False, desc=f\"Epoch {epoch}\")):\n",
        "                # Label Smoothing,\n",
        "                # i.e. if you have two target labels:\n",
        "                # Real=1 and Fake=0, then for each incoming sample, if it is real,\n",
        "                # then replace the label with a random number between 0.7 and 1.2,\n",
        "                # and if it is a fake sample, replace it with 0.0 and 0.3 (for example).\n",
        "                # Salimans et. al. 2016\n",
        "                ones = (0.7 - 1.2) * torch.rand(batch_size,\n",
        "                                                device=self.device) + 1.2\n",
        "                zeros = 0.3 * torch.rand(batch_size, device=self.device)\n",
        "\n",
        "                noise = torch.randn(size=(batch_size, 100), device=self.device)\n",
        "                # iteration for discriminator\n",
        "                z = self.G(noise, y)\n",
        "\n",
        "                pred_fake = self.D(z.detach(), y).squeeze(-1)\n",
        "                pred_real = self.D(X.to(self.device), y).squeeze(-1)\n",
        "\n",
        "                D_loss = criterion(pred_fake, zeros) + \\\n",
        "                    criterion(pred_real, ones)\n",
        "\n",
        "                D_loss.backward()\n",
        "                optimizer_D.step()\n",
        "                D_running_loss += D_loss.item() / batch_size\n",
        "\n",
        "                # iteration for the generator\n",
        "                pred = self.D(z, y).squeeze(-1)\n",
        "                G_loss = criterion(pred, ones)\n",
        "\n",
        "                G_loss.backward()\n",
        "                optimizer_G.step()\n",
        "                G_running_loss += G_loss.item() / batch_size\n",
        "\n",
        "                optimizer_D.zero_grad()\n",
        "                optimizer_G.zero_grad()\n",
        "\n",
        "                # fix noise + logging\n",
        "                if verbose and (i + 1) % 500 == 0:\n",
        "                    with torch.no_grad():\n",
        "                        print(\n",
        "                            f\"Epoch {_+1}, batch {i+1}, D_loss: {D_loss.item()}, G_loss: {G_loss.item()}\")\n",
        "                        num_imgs = 5  # Adjust the number of columns as per your preference\n",
        "                        # Adjust the figsize as per your preference\n",
        "                        _, axs = plt.subplots(1, num_imgs, figsize=(15, 5))\n",
        "\n",
        "                        # imgs = self.G(noise_fixed)\n",
        "                        imgs = self.G(noise_fixed)\n",
        "\n",
        "                        for j, img in enumerate(imgs):\n",
        "                            axs[j].imshow(\n",
        "                                img.cpu().detach().numpy().squeeze(), cmap=\"gray\")\n",
        "\n",
        "                        plt.show()\n",
        "\n",
        "            G_loss_lst.append(G_running_loss)\n",
        "            D_loss_lst.append(D_running_loss)\n",
        "\n",
        "            print(G_running_loss, D_running_loss)\n",
        "\n",
        "        return G_loss_lst, D_loss_lst\n",
        "\n",
        "    def D_step(self, X, y):\n",
        "        pass\n",
        "\n",
        "    def G_step(self, X, y):\n",
        "        pass\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b10b4be8020d4647ae97cdbbb83179b8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch:   0%|          | 0/15 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "17482f55de1d4659be005e4cef084ccc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 0:   0%|          | 0/1875 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32md:\\NUP\\ML hard\\Project\\CGAN\\main.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/NUP/ML%20hard/Project/CGAN/main.ipynb#X63sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m CGAN(device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/NUP/ML%20hard/Project/CGAN/main.ipynb#X63sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m G_loss_lst, D_loss_lst \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtrain(trainloader, EPOCHS)\n",
            "File \u001b[1;32md:\\NUP\\ML hard\\Project\\CGAN\\cgan.py:126\u001b[0m, in \u001b[0;36mCGAN.train\u001b[1;34m(self, dataloader, EPOCHS, verbose)\u001b[0m\n\u001b[0;32m    124\u001b[0m noise \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(size\u001b[39m=\u001b[39m(batch_size, \u001b[39m100\u001b[39m), device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    125\u001b[0m \u001b[39m# iteration for discriminator\u001b[39;00m\n\u001b[1;32m--> 126\u001b[0m z \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mG(noise, y)\n\u001b[0;32m    128\u001b[0m pred_fake \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mD(z\u001b[39m.\u001b[39mdetach(), y)\u001b[39m.\u001b[39msqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    129\u001b[0m pred_real \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mD(X\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice), y)\u001b[39m.\u001b[39msqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
            "File \u001b[1;32md:\\Programms\\Python3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32md:\\NUP\\ML hard\\Project\\CGAN\\cgan.py:50\u001b[0m, in \u001b[0;36mG_model.forward\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     48\u001b[0m embed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconditional(ohe)\n\u001b[0;32m     49\u001b[0m noise \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreshape_noise(X)\n\u001b[1;32m---> 50\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(torch\u001b[39m.\u001b[39;49mcat((embed, noise), dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m))\n",
            "File \u001b[1;32md:\\Programms\\Python3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32md:\\Programms\\Python3.10\\lib\\site-packages\\torch\\nn\\modules\\container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
            "File \u001b[1;32md:\\Programms\\Python3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32md:\\Programms\\Python3.10\\lib\\site-packages\\torch\\nn\\modules\\conv.py:950\u001b[0m, in \u001b[0;36mConvTranspose2d.forward\u001b[1;34m(self, input, output_size)\u001b[0m\n\u001b[0;32m    945\u001b[0m num_spatial_dims \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m    946\u001b[0m output_padding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_padding(\n\u001b[0;32m    947\u001b[0m     \u001b[39minput\u001b[39m, output_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel_size,  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    948\u001b[0m     num_spatial_dims, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv_transpose2d(\n\u001b[0;32m    951\u001b[0m     \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding,\n\u001b[0;32m    952\u001b[0m     output_padding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model = CGAN(device=device)\n",
        "G_loss_lst, D_loss_lst = model.train(trainloader, EPOCHS)\n",
        "# plot those and analyze"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
